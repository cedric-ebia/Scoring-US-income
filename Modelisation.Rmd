---
title: "Elaboration des différents modèles"
Date: "07/10/2020"
output:
  html_document:
    df_print: paged
Author: Cédric EBIA
---

# 1) Elaboration d'un modèle de régression logistique

Etant donné que nous disposons de variables d'ordre qualitatives, une première modélisation que nous pouvons effectuer est la régression logistique (Qui contrairement au modèle de régression linéaire n'est pas sensible à l'hétéroscédasticité des résidus, etc...). Les variables à utiliser pour la modélisation sont les suivantes:

    + Occupation (Relation forte >0.3)
    + Marital Status (Relation forte>0.3)
    + Education (Relation forte>0.3)
    + Hours.per.week (Relation moyenne càd entre 0.20 et 0.30)
    + Age (Relation forte>0.30)
    + Capital (Relation forte)
    + Sex
    
# Chargement des différents packages 

```{r}
#Chargement des différents packages
require(caTools)
require(ROCR)
require(pROC)
require(MASS)
require(caret)
```



(L'idée reste de construire plusieurs modèles de sorte à pouvoir faire un aller-retour vers le feature engineering afin de les améliorer)


Notre premier choix se portera sur le modèle logit

```{r}
#Détermination des différents niveaux de notre variable cible
levels(final_table$Income)
```

## 1.1) Split du jeu de données
```{r}
#Set a seed
set.seed(1234)
spl = sample.split(final_table, SplitRatio = 0.70)
train = subset(final_table, spl == TRUE)
test  = subset(final_table, spl == FALSE)
```

##1.2) Modélisation de régression logistique

  + Vérifions quelques infos sur notre échantillon d'apprentissage
```{r}
print("Pour l'échantillon d'apprentissage")
summary(train$Income)
print("Pour l'échantillon de test")
summary(test$Income)
```
  
Maintenant, on peut entamer la modélisation.



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
########################## Premier modèle à retenir ###############################
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++



## Construction d'un deuxième modèle de régression logistique (Incluant la var= Sex)

```{r}
model_glm2<-glm(Income~occupation+age+education+maritus.status+hours.per.week+Capital+sex, data=train,family = binomial)
summary(model_glm2)
```



### a) Evaluation sur TRAIN du deuxième logit

```{r}
predict2<-predict(model_glm2,train, type="response")
ROC_pred2 = prediction(predict2, train$Income)
ROC_perf2 = performance(ROC_pred2, "tpr", "fpr")
#Construction de la courbe de ROC
plot(ROC_perf2, colorize=TRUE, print.cutoffs.at = seq(0,1,0.1), text.adj = c(-0.2, 1.7))
abline(a=0, b=1)
#Adding some values
auc_train2 <- round(as.numeric(performance(ROC_pred2, "auc")@y.values),2)
legend(.8, .2, auc_train2, title = "AUC", cex=1)
#Precision recall curve
```

On peut se rendre compte qu'en dressant notre modèle, nous avons toujours un AUC de 0,89 ce qui reste relativement appréciable.

En dressant la courbe de ROC qui nous servira:
```{r}
roc_train2 = model_glm2 %>% predict(train, type="response") %>% roc(train$Income, ., plot=T)
```



```{r}
print("Détermination du seuil de probabilité à retenir(Pour comparer les modèles entre eux, nous allons considérer un seuil de 0.5")
#t_train2 = roc_train2 %>% coords("best") %>% .$threshold
#t_train2
preds_train2 = model_glm2 %>% predict(train, type="response")
preds_train2 = ifelse(preds_train2 > 0.5, 1, 0)

print("Matrice de confusion sur l'échantillon TRAIN")
matrice_confusion_train2 = preds_train2 %>% table(train$Income)
matrice_confusion_train2
print("Accuracy train")
accuracy_train2 = sum(diag(matrice_confusion_train2))/sum(matrice_confusion_train2); accuracy_train2

```


Nous avons dans ce cas une précision du modèle de 0.84 qui reste relativement convenable.
En essayant de dresser des statistiques sur ce modèle, nous obtenons les résultats suivants:

  + Sensibilité = 0,57
  + Spécificité = 0,92
  
Rappel: Seuil de probabilité = 0.5 (afin de pouvoir comparer les modèles en termes de précision entre eux selon un seuil prédéfini tout en garantissant une significativité du seuil auprès du métier).

### b) Evaluation sur l'échantillon TEST du deuxième logit

```{r}
preds_test2 = model_glm2 %>% predict(test, type="response")
preds_test2 = ifelse(preds_test2 > 0.5, 1, 0)
print("Matrice de confusion sur l'échantillon TEST")
matrice_confusion_test2 = preds_test2 %>% table(test$Income)
matrice_confusion_test2
print("Accuracy sur l'échantillon de test")
accuracy_test2 = sum(diag(matrice_confusion_test2))/sum(matrice_confusion_test2); accuracy_test2
```

Nous avons une accuracy de 0,84 sur l'échantillon test.
En essayant de dresser des statistiques sur ce modèle, nous obtenons les résultats suivants:

  + Sensibilité= 0.57
  + Spécificité= 0.92



En essayant de tracer la courbe de ROC pour l'échantillon test ainsi que la détermination de l'AUC, nous obtenons les résultats suivants:


```{r}
predict4<-predict(model_glm2,test, type="response")
ROC_pred4 = prediction(predict4, test$Income)
ROC_perf4 = performance(ROC_pred4, "tpr", "fpr")
#Construction de la courbe de ROC
plot(ROC_perf4, colorize=TRUE, print.cutoffs.at = seq(0,1,0.1), text.adj = c(-0.2, 1.7))
abline(a=0, b=1)
#Adding some values
auc_train4 <- round(as.numeric(performance(ROC_pred4, "auc")@y.values),2)
legend(.8, .2, auc_train4, title = "AUC", cex=1)
#Precision recall curve
```



En essayant de déterminer chacune des variables les plus significatives, nous obtenons les résultats suviants:


```{r}
require(car)
Anova(model_glm2)
```

Une autre façon de comparer la significativité des différentes variables:

```{r}
odds.ratio(model_glm2,level = 0.95)
```



```{r}
ratio_odd<-odds.ratio(model_glm2,level = 0.95)
ratio_odd_p<-as.data.frame(ratio_odd)
write.csv2(ratio_odd_p,"ratio_odd.csv")
```

  + Test de Wald
  
Le test de Wald confirme bien que l'ensemble de nos variables sont significatives.
Seulement , on peut se rendre compte que certaines modalités de ces variables ne sont pas significatives.
  
```{r}
#c("occupation","age","education","maritus.status","hours.per.week","Capital","sex")

print("Pour l'age")
regTermTest(model_glm2,test.terms = "age")
print("Pour la variable Occupation")
regTermTest(model_glm2,test.terms = "occupation")
print("Pour la variable education")
regTermTest(model_glm2,test.terms = "education")
print("Pour la variable marital.status")
regTermTest(model_glm2,test.terms ="maritus.status")
print("Pour la variable hours.per.week")
regTermTest(model_glm2,test.terms = "hours.per.week")
print("Pour la variable sex")
regTermTest(model_glm2,test.terms = "sex")
print("Pour la variable capital")
regTermTest(model_glm2,test.terms = "Capital")
```


```{r}
#Rapport de vraisemblance
dev<-
```


```{r}
#Recherche du meilleur seuil de proba auquel accepter que Income= >50K
# t_train2 = roc_train2 %>% coords("best") %>% .$threshold
# t_train2
# preds_train2 = model_glm2 %>% predict(train, type="response")
# preds_train2 = ifelse(preds_train2 > t_train2, 1, 0)
# 
# print("Accuracy train")
# matrice_confusion_train2 = preds_train2 %>% table(train$Income)
# #Affichage de la matrice de Confusion
# matrice_confusion_train2
# accuracy_train = sum(diag(matrice_confusion_train2))/sum(matrice_confusion_train2); accuracy_train
# #L'accuracy du modèle est de 0.78.
# 
# preds_test2 = model_glm2 %>% predict(test, type="response")
# preds_test2 = ifelse(preds_test2 > t_train2, 1, 0)
# matrice_confusion_test2 = preds_test2 %>% table(test$Income)
# print("Accuracy test")
# accuracy_test2 = sum(diag(matrice_confusion_test))/sum(matrice_confusion_test); accuracy_test
# 
# #Stocker la prédiction
# train$pred<-model_glm1 %>% predict(train, type="response")
```


# 2) Arbres de décision
```{r}
require(rpart)
require(rpart.plot)
```


## 2.1) Construction d'un premier arbre de décision

```{r}
tree1<-rpart(Income ~ occupation+age+education+maritus.status+hours.per.week+Capital+sex, data=train, method="class") #method="class" arg tells us to make classification tree and not regression tree
rpart.plot(tree1,extra = 106)
```


```{r}
prp
```

En essayant d'évaluer l'arbre de décision construit, nous obtenons les résultats suivants:

```{r}
predict_tree1_train = predict(tree1, newdata = train, type = "class")
confusionmatrix_tree1_train<-table(train$Income, predict_tree1_train)
print("Matrice de confusion de l'arbre de décision")
confusionmatrix_tree1_train
```

Au vu de la matrice de confusion sur l'échantillon d'apprentissage, nous pouvons déduire les résultats suivants:
  + Sensibilité (TRAIN)= 0,6874
  + Spécificité (TRAIN)= 0,86031

La spécificité du modèle est beaucoup plus importante que sa sensibilité.  
  
En essayant de déterminer la précision du modèle, nous obtenons les résultats suivants:

```{r}
accuract_CART1_train <- (confusionmatrix_tree1_train[1,1] + confusionmatrix_tree1_train[2,2])/sum(confusionmatrix_tree1_train)
print("Précision de l'arbre de décision construit")
accuract_CART1_train
```

On peut se rendre compte que la précision de l'arbre de décision est de 0.83 ce qui reste légèrement  inférieur à la régression logistique (sur l'échantillon d'apprentissage à première vue).

En essayant de transcrire cela sur l'échantillon test, nous obtenons les résultats suivants:

```{r}
predict_tree1_test<-predict(tree1,newdata = test, type= "class")
confusionmatrix_tree1_test<-table(predict_tree1_test,test$Income)
print("Matrice de confusion de l'arbre de décision")
confusionmatrix_tree1_test
```


Au vu de la matrice de confusion sur l'échantillon TEST, nous pouvons déduire les résultats suivants:
  + Sensibilité (TEST)= 0,47
  + Spécificité (TEST)= 0,94
  + Accuracy (TEST) = 0.83
  
On peut se rendre compte que sur l'échantillon test, l'ensemble des indicateurs sur la qualité du modèle sont relativement stables

En essayant de déterminer la précision du modèle, nous obtenons les résultats suivants:

```{r}
accuract_CART1_test <- (confusionmatrix_tree1_test[1,1] + confusionmatrix_tree1_test[2,2])/sum(confusionmatrix_tree1_test)
print("Précision de l'arbre de décision construit")
accuract_CART1_test
```

On peut se rendre compte que la précision de l'arbre de décision est de 0,83 ce qui nous permet de dire que le modèle est relativement stable.

En essayant d'évaluer les performances de l'arbre de décision, nous obtenons les résultats suivants:

```{r}
PredictROC_Tree_test = predict(tree1, newdata = test)
predict_tree_test_plot = prediction(PredictROC_Tree_test[, 2], test$Income)
#calcul de l'AUC
#On a un AUC de 0.82 sur l'arbre de décision
as.numeric(performance(predict_tree_test_plot, "auc")@y.values)
```
Sur l'échantillon TEST, nous avons un AUC de 0,82.

```{r}
#Représentation graphique de la courbe de ROC pour l'arbre de décision
performance_tree1_test = performance(predict_tree_test_plot, "tpr", "fpr")
plot(performance_tree1_test, main = "ROC curve for rpart tree on test sample")
```


En essayant de construire la courbe de ROC d'une autre façon pour l'échantillon test sur ce modèle, nous obtenons les résultats suivants:



```{r}
#Conseils utiles sur l'analyse de l'arbre de décisions
# traint = train[, c(1:(ncol(train)-1))]
# tree = rpart(Income~., data=traint)
# tree %>% summary
# tree %>% rpart.ploth   
# 
#  result_tree=tree %>% predict(traint)
# result_tree_fin<-result_tree[,-c(1)]
# head(result_tree_fin)
# #Stocker la prédiction
# predt<-tree %>% predict(traint,)
```


# 3) RANDOM FOREST

```{r}
#Chargement des packages pour appliquer l'algorithme des random Forest
require(randomForest)
require(randomForestExplainer)
require(randomForestSRC)
```


## 3.1) Construction d'un premier modèle de random forest

### a) Echantillon d'apprentissage

```{r}
train_random_forest<- randomForest(Income~occupation+age+education+maritus.status+hours.per.week+Capital+sex, data=train, importance = TRUE)
train_random_forest
```

En essayant d'analyser les différents résultats obtenus à partir de l'algorithme  random forest, nous obtenons les résultats suivants:



```{r}
#Stockage des différentes prédictions pour l'échantillon d'apprentissage
predict_train_randomforest = predict(train_random_forest, newdata = train, type = "prob")
```

Construisons la matrice de confusion pour le random forest
```{r}
confusionmatrix_rf_train<-table(train$Income, predict_train_randomforest[,2]>0.5 )
confusionmatrix_rf_train
```

En essayant de dresser quelques statistiques sur les modèles, nous obtenons les résultats suivants:
  + Sensibilité = 0.76
  + Spécificité = 0.88 
  
La précision du modèle sur l'échantillon d'apprentissage est de 0,85.

Essayons de tracer la courbe de ROC pour le random forest sur l'échantillon d'apprentissage

```{r}
# PredictROC_rf_train = predict(train_random_forest, newdata = train)
predict_rf_train_plot = prediction(predict_train_randomforest[, 2], train$Income)
#Pour obtenir l'AUC du modèle
print("AUC du modèle sur l'échantillon d'apprentissage")
as.numeric(performance(predict_rf_train_plot, "auc")@y.values)
```

Le modèle random forest sur l'échantillon d'apprentissage a un AUC de 0,89.
La précision du modèle est: Accuracy= 0,85

```{r}
#Représentation graphique de la courbe de ROC pour l'arbre de décision
performance_rf_train = performance(predict_rf_train_plot, "tpr", "fpr")
plot(performance_rf_train, main = "ROC Curve for Random Forest Model on train sample")
```

### b) Validation sur échantillon test

En appliquant le modèle sur l'échantillon de test:

```{r}
#Stockage des différentes prédictions pour l'échantillon d'apprentissage
predict_test_randomforest = predict(train_random_forest, newdata = test, type = "prob")
```

Construisons la matrice de confusion pour le random forest
```{r}
confusionmatrix_rf_test<-table(predict_test_randomforest[,2]>0.5, test$Income)
confusionmatrix_rf_test
```

Essayons de tracer la courbe de ROC pour le random forest sur l'échantillon de test

De ce fait, nous déduisons de la matrice de confusion les statistiques suivantes:

  + Précision du modèle= 0,84
  + Sensibilité= 0,72
  + Spécificité= 0,87


```{r}
# PredictROC_rf_train = predict(train_random_forest, newdata = train)
predict_rf_test_plot = prediction(predict_test_randomforest[, 2], test$Income)
#Pour obtenir l'AUC du modèle
print("AUC du modèle sur l'échantillon test")
as.numeric(performance(predict_rf_test_plot, "auc")@y.values)
```

On a un AUC de 0,87 pour le random forest.

En essayant de faire une représentation de la courbe de ROC du modèle, nous obtenons les résultats suivants:

```{r}
#Représentation graphique de la courbe de ROC pour l'arbre de décision
performance_rf_test = performance(predict_rf_test_plot, "tpr", "fpr")
plot(performance_rf_test, main = "ROC Curve for Random Forest Model on test sample")
```




```{r}
#Fréquence des variables les plus utilisées
freq_var = varUsed(train_random_forest, count=TRUE)
freq_var_used = sort(freq_var, decreasing = FALSE, index.return = TRUE)
dotchart(freq_var_used$x, names(train_random_forest$forest$xlevels[freq_var_used$ix]))
```


En essayant de vérifier l'importance des différentes variables dans nos modèles, nous obtenons les résultats suivants:

```{r}
#Graphe sur l'importance des différentes variables dans notre modélisation
varImpPlot(train_random_forest)
```








